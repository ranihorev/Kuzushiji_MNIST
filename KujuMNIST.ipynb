{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18, ResNet, BasicBlock\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from fastai.vision import * \n",
    "from fastai import *\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision.datasets.utils import makedir_exist_ok, download_url\n",
    "from torch.utils.data import BatchSampler, DataLoader\n",
    "from torchvision import transforms\n",
    "from KujuMNIST_dataset import KujuMNIST_DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean and std of the dataset for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.190362019974657\n",
      "Std: 0.3474631837640068\n"
     ]
    }
   ],
   "source": [
    "trn_data = np.load('./kuzu_mnist/kmnist-train-imgs.npz')\n",
    "trn_data = trn_data['arr_0'] / 255\n",
    "data_mean = trn_data.mean()\n",
    "data_std = trn_data.std()\n",
    "print(f'Mean: {data_mean}')\n",
    "print(f'Std: {data_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets, DataLoaders and DataBunch\n",
    "\n",
    "Optional: A random transformations on the images in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.ToPILImage(), \n",
    "     #transforms.RandomAffine(degrees=7, translate=(0.1, 0.1), scale=(0.95, 1.05)), \n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((data_mean,), (data_std,)),\n",
    "    ])\n",
    "\n",
    "transform_valid = transforms.Compose(\n",
    "    [transforms.ToPILImage(), \n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((data_mean,), (data_std,)),\n",
    "    ])\n",
    "\n",
    "ROOT_FOLDER = './kuzu_mnist/'\n",
    "\n",
    "trn_ds = KujuMNIST_DS(ROOT_FOLDER, train_or_test='train', download=False, tfms=transform_train)\n",
    "val_ds = KujuMNIST_DS(ROOT_FOLDER, train_or_test='test', download=False, tfms=transform_valid)\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "databunch = DataBunch(path=ROOT_FOLDER, train_dl=trn_dl, valid_dl=val_dl, device=default_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_transform = transforms.Compose([\n",
    "#                transforms.ToTensor(),\n",
    "#                transforms.Normalize((0.1307,), (0.3081,))\n",
    "#            ])\n",
    "\n",
    "# mnist_trn_ds = MNIST('./', train=True, download=False, transform=dataset_transform)\n",
    "# mnist_val_ds = MNIST('./', train=False, download=False, transform=dataset_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Model\n",
    "\n",
    "Based on - https://github.com/kkweon/mnist-competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):  \n",
    "    \"\"\"\n",
    "    Based on - https://github.com/kkweon/mnist-competition\n",
    "    \"\"\"\n",
    "    def two_conv_pool(self, in_channels, f1, f2):\n",
    "        s = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f1, f2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        for m in s.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        return s\n",
    "    \n",
    "    def three_conv_pool(self,in_channels, f1, f2, f3):\n",
    "        s = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f1, f2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f2, f3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(f3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        for m in s.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        return s\n",
    "        \n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.l1 = self.two_conv_pool(1, 64, 64)\n",
    "        self.l2 = self.two_conv_pool(64, 128, 128)\n",
    "        self.l3 = self.three_conv_pool(128, 256, 256, 256)\n",
    "        self.l4 = self.three_conv_pool(256, 256, 256, 256)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x, dim=1) \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Model\n",
    "\n",
    "Based on - https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "class MyResNet(nn.Module):\n",
    "    # Based on PyTorch ResNet-18\n",
    "    \n",
    "    def __init__(self, block, layers, num_classes=10, zero_init_residual=False):\n",
    "        super(MyResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "                    \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512 * block.expansion, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of VGG and ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_ResNet, self).__init__()\n",
    "        self.vgg = VGG()\n",
    "        self.resnet = MyResNet(BasicBlock, [2, 2, 2, 2])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        vgg_out = self.vgg(x)\n",
    "        resnet_out = self.resnet(x)\n",
    "        out = (vgg_out + resnet_out) / 2\n",
    "        return out\n",
    "    \n",
    "def vgg_resnet_load_model(learner, vgg_name, resnet_name):\n",
    "        device = learner.data.device\n",
    "        vgg_state = torch.load(learner.path/learner.model_dir/f'{vgg_name}.pth', map_location=device)\n",
    "        learner.model.vgg.load_state_dict(vgg_state['model'], strict=True)\n",
    "        \n",
    "        resnet_state = torch.load(learner.path/learner.model_dir/f'{resnet_name}.pth', map_location=device)\n",
    "        learner.model.resnet.load_state_dict(resnet_state['model'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was loaded\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(databunch, VGG_ResNet(), metrics=accuracy)\n",
    "# vgg_resnet_load_model(learn, vgg_name, resnet_name)\n",
    "learn.load('vgg_resnet_model_with_norm')\n",
    "print('Model was loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 03:21 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.015457</th>\n",
       "    <th>0.092744</th>\n",
       "    <th>0.989000</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capsule Network\n",
    "\n",
    "Taken from - https://github.com/higgsfield/Capsule-Network-Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=256, kernel_size=9):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=1\n",
    "                             )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "    \n",
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0) \n",
    "                          for _ in range(num_capsules)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), 32 * 6 * 6, -1)\n",
    "        return self.squash(u)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "    \n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
    "        \n",
    "        b_ij = b_ij.to(default_device)\n",
    "\n",
    "        num_iterations = 3\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, data):\n",
    "        classes = torch.sqrt((x ** 2).sum(2))\n",
    "        classes = F.softmax(classes)\n",
    "        \n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        masked = Variable(torch.eye(10))\n",
    "\n",
    "        masked = masked.to(default_device)\n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n",
    "        \n",
    "        reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n",
    "        reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
    "        \n",
    "        return reconstructions, masked\n",
    "\n",
    "    \n",
    "def caps_loss(inputs, targets):\n",
    "    targets = torch.eye(10).index_select(dim=0, index=targets.cpu()).to(default_device)\n",
    "    data, output, reconstructions, masked = inputs\n",
    "    return margin_loss(output, targets) + reconstruction_loss(data, reconstructions)\n",
    "    \n",
    "def margin_loss(x, labels, size_average=True):\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "    left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "    right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "    \n",
    "    loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "    loss = loss.sum(dim=1).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def reconstruction_loss(data, reconstructions):\n",
    "    loss = F.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "    return loss * 0.0005\n",
    "\n",
    "def caps_accuracy(inputs, targs):\n",
    "    masked = inputs[-1]\n",
    "    predictions = np.argmax(masked.data.cpu().numpy(), 1)\n",
    "    return torch.tensor((predictions == targs.cpu().numpy()).mean())\n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n",
    "        reconstructions, masked = self.decoder(output, data)\n",
    "        return data, output, reconstructions, masked\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was loaded\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(databunch, CapsNet(), metrics=caps_accuracy, loss_func=caps_loss)\n",
    "# vgg_resnet_load_model(learn, vgg_name, resnet_name)\n",
    "learn.load('caps_net_model_with_norm')\n",
    "print('Model was loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 06:35 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>caps_accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.006563</th>\n",
       "    <th>0.071374</th>\n",
       "    <th>0.976700</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of VGG and Capsule Network\n",
    "\n",
    "Results are worse than VGG-ResNet Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_Caps(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_Caps, self).__init__()\n",
    "        self.vgg = VGG()\n",
    "        caps_model = CapsNet()\n",
    "        self.capsnet = caps_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        vgg_out = self.vgg(x)\n",
    "        capsnet_out = self.capsnet(x)\n",
    "        return vgg_out, capsnet_out\n",
    "\n",
    "    \n",
    "def vgg_capsnet_load_model(learner, vgg_name, caps_name):\n",
    "        device = learner.data.device\n",
    "        vgg_state = torch.load(learner.path/learner.model_dir/f'{vgg_name}.pth', map_location=device)\n",
    "        learner.model.vgg.load_state_dict(vgg_state['model'], strict=True)\n",
    "        \n",
    "        capsnet_state = torch.load(learner.path/learner.model_dir/f'{caps_name}.pth', map_location=device)\n",
    "        learner.model.capsnet.load_state_dict(capsnet_state['model'], strict=True)\n",
    "\n",
    "def vgg_caps_accuracy(outputs, targs):\n",
    "    caps_outputs = outputs[1][-1]\n",
    "    vgg_outputs = outputs[0]\n",
    "    batch_size = targs.size(0)    \n",
    "    \n",
    "    caps_outputs = F.softmax(caps_outputs, dim=1)\n",
    "    \n",
    "    final_preds = (caps_outputs + vgg_outputs) / 2\n",
    "    final_preds = final_preds.argmax(dim=-1).view(batch_size,-1)\n",
    "\n",
    "    targs = targs.view(batch_size,-1)\n",
    "    return (final_preds==targs).float().mean()\n",
    "    \n",
    "def vgg_caps_loss(inputs, targets):\n",
    "    vgg_loss = torch.functional.F.nll_loss(inputs[0], targets)\n",
    "    return caps_loss(inputs[1], targets) + vgg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(databunch, VGG_Caps(), metrics=vgg_caps_accuracy, loss_func=vgg_caps_loss)\n",
    "vgg_capsnet_load_model(learn, 'vgg_model_with_norm', 'caps_net_model_with_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 07:42 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>vgg_caps_accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.022337</th>\n",
       "    <th>0.150881</th>\n",
       "    <th>0.986300</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "learn.fit(1)\n",
    "# learn.save('vgg_resnet_model_with_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
